{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b146ba-08b6-4adb-a858-8e4294c5e781",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Fine-tune the teacher on the dataset\n",
    "\n",
    "NeMo Framework includes a standard Python script, [megatron_gpt_pretraining.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py), for training a model. Once you have your model downloaded and the dataset ready, fine-tuning the teacher model with NeMo is essentially just running this script!\n",
    "\n",
    "We fine-tune the unpruned model on our dataset to correct the distribution shift from the original dataset the model was trained on. According to the [blog](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/) and [tech report](https://arxiv.org/pdf/2408.11796), experiments showed that without correcting for this distribution shift, the teacher provides suboptimal guidance on the dataset during distillation.\n",
    "\n",
    "For this demonstration, this training run is capped by `STEPS`, and validation is carried out every `VAL_INTERVAL` steps.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your pre-processed train, test, and validation data files, as well as the path to the teacher .nemo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12007ac8-2fd5-4de8-8964-97821c2198c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
    "\n",
    "# Set path(s) if different:\n",
    "\n",
    "MODEL=\"/workspace/llama-3_1-8b-nemo_v1.0/llama3_1_8b.nemo\"\n",
    "\n",
    "# Can change these to accommodate resources:\n",
    "\n",
    "TENSOR_PARALLEL_SIZE=8\n",
    "NODES=1\n",
    "MICRO_BATCH_SIZE=4\n",
    "\n",
    "# Don't change the following:\n",
    "\n",
    "EXPERIMENT_DIR=\"distill_trainings\"\n",
    "EXPERIMENT_NAME=\"megatron_llama_ft\"\n",
    "\n",
    "DATA_TRAIN='wikitext_tokenized_train_text_document'\n",
    "DATA_VAL='wikitext_tokenized_test_text_document'\n",
    "DATA_TEST='wikitext_tokenized_val_text_document'\n",
    "\n",
    "STEPS=30\n",
    "GLOBAL_BATCH_SIZE=128\n",
    "\n",
    "LOG_INTERVAL=1\n",
    "VAL_INTERVAL=10\n",
    "NUM_VAL_BATCHES=5\n",
    "\n",
    "LR=1e-4\n",
    "MIN_LR=1e-5\n",
    "WARMUP_STEPS=2\n",
    "\n",
    "cmd=\"torchrun --nproc-per-node=${TENSOR_PARALLEL_SIZE}\"\n",
    "\n",
    "${cmd} /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "    --config-path /opt/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "    --config-name megatron_llama_distill.yaml \\\n",
    "    \\\n",
    "    name=${EXPERIMENT_NAME} \\\n",
    "    \\\n",
    "    exp_manager.exp_dir=${EXPERIMENT_DIR} \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "    exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "    \\\n",
    "    trainer.max_steps=${STEPS} \\\n",
    "    trainer.log_every_n_steps=${LOG_INTERVAL} \\\n",
    "    trainer.val_check_interval=${VAL_INTERVAL} \\\n",
    "    trainer.limit_val_batches=${NUM_VAL_BATCHES} \\\n",
    "    +trainer.num_sanity_val_steps=0 \\\n",
    "    \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.devices=${TENSOR_PARALLEL_SIZE} \\\n",
    "    trainer.num_nodes=${NODES} \\\n",
    "    \\\n",
    "    \"model.data.data_prefix={train:[1.0,$DATA_TRAIN],validation:[$DATA_VAL],test:[$DATA_TEST]}\" \\\n",
    "    \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    +model.dist_ckpt_load_strictness=log_all \\\n",
    "    \\\n",
    "    ~model.tokenizer \\\n",
    "    +model.tokenizer='{library: huggingface, type: meta-llama/Meta-Llama-3.1-8B, use_fast: True}' \\\n",
    "    \\\n",
    "    model.tensor_model_parallel_size=${TENSOR_PARALLEL_SIZE} \\\n",
    "    model.sequence_parallel=True \\\n",
    "    model.micro_batch_size=${MICRO_BATCH_SIZE} \\\n",
    "    model.global_batch_size=${GLOBAL_BATCH_SIZE} \\\n",
    "    \\\n",
    "    model.encoder_seq_length=8192 \\\n",
    "    model.num_layers=32 \\\n",
    "    model.hidden_size=4096 \\\n",
    "    model.ffn_hidden_size=14336 \\\n",
    "    model.num_attention_heads=32 \\\n",
    "    model.hidden_dropout=0.0 \\\n",
    "    model.attention_dropout=0.0 \\\n",
    "    model.apply_query_key_layer_scaling=True \\\n",
    "    model.normalization='rmsnorm' \\\n",
    "    model.bias=False \\\n",
    "    model.activation='fast-swiglu' \\\n",
    "    model.position_embedding_type='rope' \\\n",
    "    model.share_embeddings_and_output_weights=False \\\n",
    "    model.num_query_groups=8 \\\n",
    "    ++model.scale_positional_embedding=True \\\n",
    "    ++model.rotary_base=500000.0 \\\n",
    "    \\\n",
    "    model.optim.name=distributed_fused_adam \\\n",
    "    model.optim.lr=${LR} \\\n",
    "    model.optim.sched.min_lr=${MIN_LR} \\\n",
    "    model.optim.sched.warmup_steps=${WARMUP_STEPS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040a993-8423-475f-8bc6-d1dd1ce16a83",
   "metadata": {},
   "source": [
    "This will create a fine-tuned teacher model named `megatron_llama_ft.nemo` in `./distill_trainings/megatron_llama_ft/checkpoints/`. We'll use this later.\n",
    "> `NOTE:`This script takes at least 20 minutes to run (depending on GPU) and will generate the fine-tuned teacher model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
