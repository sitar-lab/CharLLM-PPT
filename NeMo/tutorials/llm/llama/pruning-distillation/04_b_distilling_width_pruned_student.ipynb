{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5062f23-c604-479b-9a4e-69989598b131",
   "metadata": {},
   "source": [
    "### Step 4: Distill knowledge from teacher into student\n",
    "Distillation of a model with NeMo Framework is also possible using a Python script: [megatron_gpt_distillation.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_distillation.py). \n",
    "In this notebook, we will explore distillation with the width-pruned model as the `STUDENT` model.\n",
    "\n",
    "For this demonstration, the `TEACHER` would be the fine-tuned teacher model `megatron_llama_ft.nemo` and the `STUDENT` model would be the pruned 4B model. This training run is capped by `STEPS`, and validation is carried out every `VAL_INTERVAL` steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7de691-dd1d-4719-9872-98501a22e3c9",
   "metadata": {},
   "source": [
    "#### Step 4.b.: Using width-pruned student\n",
    "While distilling knowledge from the teacher to width-pruned model, the `STUDENT` model would be  `4b_width_pruned_model.nemo` as produced by the [width-pruning](./03_b_width_pruning.ipynb) notebook. This training run is capped by `STEPS`, and validation is carried out every `VAL_INTERVAL` steps.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your pre-processed train, test, and validation data files, as well as path to the teacher and student .nemo models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070b526-771a-4a8d-b0ba-ab218b382bd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
    "\n",
    "# Can change these to accommodate resources:\n",
    "\n",
    "TENSOR_PARALLEL_SIZE=8\n",
    "NODES=1\n",
    "MICRO_BATCH_SIZE=4\n",
    "\n",
    "# Don't change the following:\n",
    "\n",
    "EXPERIMENT_DIR=\"distill_trainings\"\n",
    "EXPERIMENT_NAME=\"megatron_llama_distill_width_pruned_student\"\n",
    "\n",
    "TEACHER=\"${EXPERIMENT_DIR}/megatron_llama_ft/checkpoints/megatron_llama_ft.nemo\"\n",
    "STUDENT=\"/workspace/4b_width_pruned_model.nemo\"\n",
    "\n",
    "FINAL_MODEL_PATH=\"${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/checkpoints/width_pruned_distilled_4b_model.nemo\"\n",
    "\n",
    "DATA_TRAIN='wikitext_tokenized_train_text_document'\n",
    "DATA_VAL='wikitext_tokenized_test_text_document'\n",
    "DATA_TEST='wikitext_tokenized_val_text_document'\n",
    "\n",
    "STEPS=30\n",
    "GLOBAL_BATCH_SIZE=128\n",
    "\n",
    "LOG_INTERVAL=1\n",
    "VAL_INTERVAL=10\n",
    "NUM_VAL_BATCHES=5\n",
    "\n",
    "LR=1e-4\n",
    "MIN_LR=1e-5\n",
    "WARMUP_STEPS=2\n",
    "\n",
    "cmd=\"torchrun --nproc-per-node=${TENSOR_PARALLEL_SIZE}\"\n",
    "\n",
    "${cmd} /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_distillation.py \\\n",
    "    name=${EXPERIMENT_NAME} \\\n",
    "    \\\n",
    "    exp_manager.exp_dir=${EXPERIMENT_DIR} \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "    \\\n",
    "    trainer.max_steps=${STEPS} \\\n",
    "    trainer.log_every_n_steps=${LOG_INTERVAL} \\\n",
    "    trainer.val_check_interval=${VAL_INTERVAL} \\\n",
    "    trainer.limit_val_batches=${NUM_VAL_BATCHES} \\\n",
    "    +trainer.num_sanity_val_steps=0 \\\n",
    "    \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.devices=${TENSOR_PARALLEL_SIZE} \\\n",
    "    trainer.num_nodes=${NODES} \\\n",
    "    \\\n",
    "    \"model.data.data_prefix={train:[1.0,$DATA_TRAIN],validation:[$DATA_VAL],test:[$DATA_TEST]}\" \\\n",
    "    \\\n",
    "    model.restore_from_path=${STUDENT} \\\n",
    "    model.kd_teacher_restore_from_path=${TEACHER} \\\n",
    "    model.nemo_path=${FINAL_MODEL_PATH} \\\n",
    "    \\\n",
    "    model.tensor_model_parallel_size=${TENSOR_PARALLEL_SIZE} \\\n",
    "    model.sequence_parallel=True \\\n",
    "    model.micro_batch_size=${MICRO_BATCH_SIZE} \\\n",
    "    model.global_batch_size=${GLOBAL_BATCH_SIZE} \\\n",
    "    \\\n",
    "    model.optim.name=distributed_fused_adam \\\n",
    "    model.optim.lr=${LR} \\\n",
    "    model.optim.sched.min_lr=${MIN_LR} \\\n",
    "    model.optim.sched.warmup_steps=${WARMUP_STEPS} \\\n",
    "    +model.dist_ckpt_load_strictness=log_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbc377-e19a-49e0-b245-fa828cca415a",
   "metadata": {},
   "source": [
    "This will create the final width-pruned distilled model named `width_pruned_distilled_4b_model.nemo` in `./distill_trainings/megatron_llama_distill_width_pruned_student/checkpoints`.\n",
    "> `NOTE:`This script takes at least 20 minutes to run (depends on GPU) and generate the final distilled model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
